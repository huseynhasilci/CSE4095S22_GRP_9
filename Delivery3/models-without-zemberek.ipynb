{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf091af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.debugger import set_trace\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import tensorflow as tf\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "plt.style.use(style=\"seaborn\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175044f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_csv_file = \"latest_hukums_with_classes_csv_file1.csv\"\n",
    "df = pd.read_csv(path_csv_file)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6957deb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {}\n",
    "count = 0\n",
    "for label in df.YeniSuclar.unique():\n",
    "    label_map[label] = count\n",
    "    count += 1\n",
    "df['NUM_LABEL'] = df.YeniSuclar.map(label_map)\n",
    "#print(label_map)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170dff0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(how='any',axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd5cb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55ef468",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    alphabetic_only = [word for word in text.split() if word.isalpha()]\n",
    "    lower_case_only = [word.lower() for word in alphabetic_only]\n",
    "    stopwords_tr = set(stopwords.words(\"turkish\"))    \n",
    "    return [word for word in lower_case_only if word not in stopwords_tr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb0cc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[:, 2] = df.iloc[:, 2].apply(lambda x: \" \".join(clean_text(x)))\n",
    "df = df[['ictihat', 'NUM_LABEL']]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a972d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.ictihat\n",
    "Y = df.NUM_LABEL\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf086de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,Y, test_size=0.2, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5968e0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_df = X_train.str.split(expand=True).stack().value_counts().reset_index()\n",
    " \n",
    "freq_df.columns = ['Word', 'Frequency'] \n",
    " \n",
    "freq_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b727c4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_v_size = len(freq_df)\n",
    "train_v_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c441a5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit tokenizer on training data\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "#get train sequences\n",
    "train_seqs = tokenizer.texts_to_sequences(X_train)\n",
    "train_seqs_max_size = max([len(seq) for seq in train_seqs])\n",
    "#get test sequences\n",
    "test_seqs = tokenizer.texts_to_sequences(X_test)\n",
    "test_seqs_max_size = max([len(seq) for seq in test_seqs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4dad43",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_padded = pad_sequences(train_seqs, maxlen=train_seqs_max_size, padding=\"post\", truncating=\"post\")\n",
    "test_padded = pad_sequences(test_seqs, maxlen=train_seqs_max_size, padding=\"post\", truncating=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f792fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seqs_max_size, test_seqs_max_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b497ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tokenized = [[word for word in document.split()] for document in X_train]\n",
    "X_train_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c718b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec, FastText\n",
    "word_model = Word2Vec(X_train_tokenized, vector_size=100)\n",
    "\n",
    "#build matrix \n",
    "embedding_matrix_w2v = np.random.random(((train_v_size) + 1, 100))\n",
    "for word,i in tokenizer.word_index.items():  \n",
    "    try:\n",
    "        embedding_matrix_w2v[i] = word_model.wv[word]\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# create layer\n",
    "embedding_layer_w2v = Embedding((train_v_size) + 1, output_dim=100, \n",
    "                            weights=[embedding_matrix_w2v], trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9a91cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft = FastText(vector_size=300)\n",
    "ft.build_vocab(X_train_tokenized)\n",
    "ft.train(tokenizer.word_index, total_examples=ft.corpus_count, epochs=10)\n",
    "\n",
    "# build matrix\n",
    "embedding_matrix_ft = np.random.random(((train_v_size) + 1, ft.vector_size))\n",
    "for word,i in tokenizer.word_index.items(): \n",
    "    try:\n",
    "        embedding_matrix_ft[i] = ft.wv[word]\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# create layer\n",
    "embedding_layer_ft = Embedding((train_v_size) + 1, output_dim=300, \n",
    "                            weights=[embedding_matrix_ft], trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c76435f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout, Conv1D, GlobalMaxPooling1D\n",
    "from keras.initializers import Constant\n",
    "from keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.regularizers import L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d932b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_model(embeddings, classification=True):\n",
    "    model = Sequential()\n",
    "    model.add(embeddings)\n",
    "    model.add(LSTM(64, dropout=0.1))\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "    adam_opt = Adam(learning_rate=3e-4)\n",
    "    if classification:\n",
    "        model.compile(loss=\"binary_crossentropy\", optimizer=adam_opt, metrics=[\"accuracy\"])\n",
    "    else: \n",
    "        model.compile(loss=\"mean_squared_error\", optimizer=adam_opt, metrics=[\"mse\"])\n",
    "        \n",
    "    return model \n",
    "\n",
    "def train_model(model, train_padded, test_padded, y_train, y_test):\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "    history = model.fit( train_padded, y_train, epochs=20,\n",
    "                            validation_data=(test_padded, y_test), callbacks=[early_stopping])\n",
    "    \n",
    "    return history\n",
    "    \n",
    "    \n",
    "def evaluate_model(model, test_padded, y_test):\n",
    "    results = model.evaluate(test_padded, y_test, batch_size=128)\n",
    "    return results \n",
    "\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b319e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lstm_model(embedding_layer_w2v)    \n",
    "history = train_model(model, train_padded, test_padded, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5022e045",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_lstm_status_history = pd.DataFrame(history.history)\n",
    "w2v_lstm_status_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6766c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(model, test_padded, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb81603b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "419/482 [=========================>....] - ETA: 5:37 - loss: -51.6407 - accuracy: 0.1495"
     ]
    }
   ],
   "source": [
    "model = lstm_model(embedding_layer_ft)    \n",
    "history = train_model(model, train_padded, test_padded, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107bc985",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_lstm_status_history = pd.DataFrame(history.history)\n",
    "ft_lstm_status_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc859ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d5edb8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
